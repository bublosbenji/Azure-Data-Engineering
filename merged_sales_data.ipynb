{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4ed9c0-11b7-4111-8aeb-41361fb79fa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a databricks secret scope linked to Azure key vault\n",
    "databricks secrets create-scope --scope azure-keyvault --scope-backend-type AZURE_KEYVAULT --resource-id </subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.KeyVault/vaults/<key-vault-name>> --dns-name <https://sales-data-key-vault.vault.azure.net/>\n",
    "\n",
    "# mount ADLS Gen2 filesystem\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": dbutils.secrets.get(scope=\"azure-keyvault\", key=\"application-client-id\"),\n",
    "    \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"azure-keyvault\", key=\"secret-key\"),\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{dbutils.secrets.get(scope='azure-keyvault', key='directory-tenant-id')}/oauth2/token\"\n",
    "}\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=\"abfss://sales-data@salesdatabenji.dfs.core.windows.net\",\n",
    "    mount_point=\"/mnt/salesdata\",\n",
    "    extra_configs=configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f91005d3-6340-4173-a973-71b3d291a5e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  list the contents of a specified directory\n",
    "%fs ls \"/mnt/salesdata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea707a89-ef1c-4343-8c1a-59ae148eefde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load our data\n",
    "merged_data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/mnt/salesdata/raw-data/merged_sales_data.csv\")\n",
    "merged_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6c829b-f144-4ec8-aa93-75192347413c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# count null values in each column\n",
    "null_counts = {col: merged_data.filter(merged_data[col].isNull()).count() for col in merged_data.columns}\n",
    "print(null_counts)\n",
    "\n",
    "# get the total number of null values across all columns\n",
    "total_nulls = sum(null_counts.values())\n",
    "print(total_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a314f2a-e36f-42ab-9d02-3745a224c562",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop null values in each column\n",
    "merged_data_cleaned = merged_data.na.drop()\n",
    "merged_data_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfb060a-0845-42f1-940b-9f30bf29d286",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "merged_data_cleaned = merged_data_cleaned.withColumn(\"Quantity Ordered\", col(\"Quantity Ordered\").cast(\"int\"))\n",
    "merged_data_cleaned = merged_data_cleaned.withColumn(\"Price Each\", col(\"Price Each\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8986ac17-b379-405a-b7ab-078d0974492d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cleaned data save to a different path\n",
    "merged_data_cleaned.repartition(1).write.mode(\"overwrite\").option(\"header\",'true').csv(\"/mnt/salesdata/cleaned-data/merged_sales_data\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "merged_sales_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
